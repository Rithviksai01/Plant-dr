# -*- coding: utf-8 -*-
"""Disease detection ai .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/159e3wN0AJkhPoOA1R9HawjO-2RR15lz1
"""

!pip install -q kaggle

import kagglehub

# Download latest version
path = kagglehub.dataset_download("alexanderuzhinskiy/the-doctorp-project-dataset")

print("Path to dataset files:", path)

import os

# Use the 'path' variable returned by kagglehub.dataset_download
# This variable holds the actual path to the downloaded dataset
dataset_path = path
print("Classes:", os.listdir(dataset_path))

from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os
import kagglehub

# Download latest version
path = kagglehub.dataset_download("alexanderuzhinskiy/the-doctorp-project-dataset")

print("Path to dataset files:", path)

# Use the 'path' variable returned by kagglehub.dataset_download
# This variable holds the actual path to the downloaded dataset
# The actual dataset content might be in a subdirectory. Based on the error,
# it seems to be in a folder named 'doctorp_dataset' inside the downloaded path.
dataset_root_path = path # This is the downloaded location

# **Modification Start**
# Instead of assuming a subdirectory, we will use the root path directly
# unless inspection of the downloaded content suggests otherwise.
# The error message and the initial print of os.listdir(dataset_path) in a previous cell
# likely indicate the classes are directly within the root path.
dataset_path_for_generators = dataset_root_path

# We can remove the explicit check for 'doctorp_dataset' if we confirm the structure.
# If you still suspect a subdirectory, uncomment the lines below and
# adjust 'doctorp_dataset' based on the output of os.listdir(dataset_root_path).

# actual_dataset_content_path = os.path.join(dataset_root_path, 'doctorp_dataset')
# # Check if the assumed path exists before proceeding
# if not os.path.isdir(actual_dataset_content_path):
#     print(f"Error: Directory not found at {actual_dataset_content_path}")
#     print("Please inspect the contents of the downloaded path:", os.listdir(dataset_root_path))
#     # You might need to adjust 'doctorp_dataset' based on the actual content
# else:
#     dataset_path_for_generators = actual_dataset_content_path
# **Modification End**


# Use the corrected path for the generators
print("Classes:", os.listdir(dataset_path_for_generators))

img_height, img_width = 256, 256
batch_size = 32

train_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,  # 20% for validation
    horizontal_flip=True,
    zoom_range=0.2,
    rotation_range=20
)

# Use the corrected dataset_path_for_generators
train_generator = train_datagen.flow_from_directory(
    dataset_path_for_generators,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical',
    subset='training'
)

# Use the corrected dataset_path_for_generators
validation_generator = train_datagen.flow_from_directory(
    dataset_path_for_generators,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation'
)

import tensorflow as tf
from tensorflow.keras import layers, models

model = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, 3)),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(128, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(train_generator.num_classes, activation='softmax')
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

epochs = 10
history = model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=epochs
)

import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss= history.history['loss']
val_loss= history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

model.save("doctorp_model.h5")

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with open('doctorp_model.tflite', 'wb') as f:
    f.write(tflite_model)

import os
from google.colab import files

# Check if the file exists before attempting to download
file_to_download = "doctorp_model.tflite"
if os.path.exists(file_to_download):
    print(f"File '{file_to_download}' found. Attempting download...")
    files.download(file_to_download)
else:
    print(f"Error: File '{file_to_download}' not found. The TFLite model was likely not created successfully.")